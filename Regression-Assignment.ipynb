{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-What is Simple Linear Regression\n",
    "#Ans-Simple Linear Regression is a statistical method used to find the relationship between two variables\n",
    "\n",
    "# one independent variable (X) and one dependent variable (y). \n",
    "\n",
    "# It fits a straight line (y = mX + b) to the data, where \"m\" is the slope, and \"b\" is the y-intercept\n",
    "\n",
    "# which shows how changes in X predict changes in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2- What are the key assumptions of Simple Linear Regression\n",
    "\n",
    "#The key assumptions of Simple Linear Regression are:\n",
    "\n",
    "#Linearity: The relationship between the independent (x) and dependent (y) variables is linear.\n",
    "\n",
    "#Independence: The observations are independent of each other.\n",
    "\n",
    "#Homoscedasticity: The variance of errors (difference between predicted and actual values) is constant across all levels of x.\n",
    "\n",
    "#Normality: The errors follow a normal distribution\n",
    "\n",
    "#No Multicollinearity: In simple linear regression, there should be only one independent variable,  multicollinearity doesn't apply directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-  What does the coefficient m represent in the equation Y=mX+c\n",
    "\n",
    "#The coefficient m in the equation y=mX+c represents the slope of the line.\n",
    "\n",
    "#  It shows how much the dependent variable (y) changes for a one-unit increase in the independent variable (X).\n",
    "\n",
    "#  it indicates the rate of change or the relationship between X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4-What does the intercept c represent in the equation Y=mX+c\n",
    "\n",
    "# the intercept c in the equation Y=mX+c \n",
    "\n",
    "# represents the baseline prediction of the model when the independent variable (X) is 0.\n",
    "\n",
    "# It indicates the starting value of the dependent variable (y) before considering the effect of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5- How do we calculate the slope m in Simple Linear Regression\n",
    "\n",
    "#way to calculate the slope\n",
    "#pick two points on the line(x1,y1) and(x2,y2).\n",
    "\n",
    "#formula:      m=y2-y1/x2-x1\n",
    "\n",
    "#this means how much y changes for a change in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6-What is the purpose of the least squares method in Simple Linear Regression\n",
    "\n",
    "#The least squares method in Simple Linear Regression finds the line that best fits the data by\n",
    "\n",
    "# minimizing the sum of the squared differences (errors) between the actual values and the predicted values. \n",
    "\n",
    "# It ensures the model makes the smallest overall error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7-  - How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
    "\n",
    "#The coefficient of determination R² in Simple Linear Regression is interpreted as:\n",
    "\n",
    "#It measures the proportion of the variation in the dependent variable (Y) explained by the independent variable (X).\n",
    "\n",
    "#ranges from 0 to 1, where 1 means the model explains all the variation, and 0 means it explains none.\n",
    "\n",
    "# Higher R² indicates a better fit of the regression line to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-What is Multiple Linear Regression\n",
    "\n",
    "#Multiple Linear Regression is a statistical method used to predict \n",
    "#dependent variable(y) based on two or more independent variables (X1,X2,X3,etc)\n",
    "\n",
    "#It fits a line to the data using the eqn\n",
    "# y=c0+c1X1 +c2X2+c3X3+.....+cnXn\n",
    "#shows how each variable influences the outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9-What is the main difference between Simple and Multiple Linear Regression\n",
    "\n",
    "#Simple Linear Regression uses only one independent variable to predict the dependent variable.\n",
    "\n",
    "#Multiple Linear Regression uses two or more independent variables to predict the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-What are the key assumptions of Multiple Linear Regression\n",
    "#The key assumptions of Multiple Linear Regression are:\n",
    "\n",
    "#Linearity: The relationship between the dependent variable and each independent variable is linear.\n",
    "\n",
    "#Independence: The observations are independent of each other.\n",
    "\n",
    "#Homoscedasticity: The variance of errors is constant across all levels of the independent variables.\n",
    "\n",
    "#Normality: The errors follow a normal distribution.\n",
    "\n",
    "#No Multicollinearity: The independent variables should not be highly correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11-- What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
    "\n",
    "#Heteroscedasticity means the variance of errors (residuals) is not constant across all levels of the independent variables.\n",
    "\n",
    "#It violates the assumption of homoscedasticity in regression models.\n",
    "\n",
    "#This can lead to biased standard errors, making hypothesis tests and confidence intervals unreliable.\n",
    "\n",
    "#The model may misinterpret the significance of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12- How can you improve a Multiple Linear Regression model with high multicollinearity\n",
    "#To improve a Multiple Linear Regression model with high multicollinearity:\n",
    "\n",
    "#Remove highly correlated variables to reduce redundancy.\n",
    "\n",
    "#Use regularization techniques like Ridge or Lasso Regression.\n",
    "\n",
    "#Combine correlated variables using Principal Component Analysis (PCA) or create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13-Common techniques for transforming categorical variables are:\n",
    "\n",
    "#One-Hot Encoding: Converts categories into separate binary columns (0 or 1).\n",
    "\n",
    "#Label Encoding: Assigns a unique number to each category.\n",
    "\n",
    "#Dummy Encoding: Similar to one-hot but drops one column to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14-What is the role of interaction terms in Multiple Linear Regression\n",
    "\n",
    "#Interaction terms in Multiple Linear Regression show how the effect of one \n",
    "# independent variable on the dependent variable changes based on another independent variable.\n",
    "\n",
    "#They capture combined effects of two variables working together.\n",
    "\n",
    "#Example: If studying hours and sleep interact, the combined effect on test scores is modeled.\n",
    "\n",
    "#They help improve the model when variables influence each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15-How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
    "\n",
    "#In Simple Linear Regression, the intercept is the predicted value of the dependent variable when the independent variable is 0.\n",
    "\n",
    "#In Multiple Linear Regression, the intercept represents the predicted value of the dependent variable when all independent variables are 0.\n",
    "\n",
    "#In multiple regression, it may have less practical meaning if variables like age or income can’t logically be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16-What is the significance of the slope in regression analysis,\n",
    "#  and how does it affect predictions\n",
    "\n",
    "#The slope in regression analysis shows the rate of change in the dependent variable (Y) for a one-unit increase in the independent variable (X).\n",
    "\n",
    "#A positive slope means Y increases as X increases; a negative slope means Y decreases as X increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17- How does the intercept in a regression model provide context for the relationship between variables\n",
    "\n",
    "#The intercept in a regression model provides the starting point of the \n",
    "# relationship between variables\n",
    "\n",
    "#It represents the value of the dependent variable (Y)\n",
    "#  when all independent variables (X) are 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18-What are the limitations of using R² as a sole measure of model performance\n",
    "\n",
    "#limitations of using as a sole measure of model performance are:\n",
    "\n",
    "#doesn’t account for overfitting, so a higher value doesn’t always mean a better model.\n",
    "\n",
    "#It doesn’t work well with non-linear relationships or models with irrelevant variables.\n",
    "\n",
    "#It doesn’t measure predictive accuracy; other metrics like RMSE or MAE are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19-How would you interpret a large standard error for a regression coefficient\n",
    "\n",
    "#A large standard error for a regression coefficient means:\n",
    "\n",
    "#The estimate of the coefficient is unstable and varies a lot across different samples.\n",
    "\n",
    "#The variable may have a weak or uncertain relationship with the dependent variable.\n",
    "\n",
    "#It can make the coefficient statistically insignificant, reducing confidence in its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20-How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
    "\n",
    "#Identifying Heteroscedasticity: In residual plots, heteroscedasticity appears as a non-uniform pattern, \n",
    "\n",
    "# like a \"funnel shape,\" where residuals spread wider as the predicted values increase.\n",
    "\n",
    "#Importance: It violates regression assumptions leading to biased standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
    "\n",
    "#It means the model includes irrelevant variables that don’t improve predictions.\n",
    "\n",
    "#increases with more variables, but adjusted \n",
    "\n",
    "#decreases for unnecessary ones.\n",
    "\n",
    "#The model might be overfitting, reducing its reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22-Why is it important to scale variables in Multiple Linear Regression\n",
    "\n",
    "#It ensures all variables are on the same scale, preventing larger values from dominating the model.\n",
    "\n",
    "#It improves the performance of algorithms like gradient descent .\n",
    "\n",
    "#It helps interpret coefficients accurately when variables have different units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#23-- What is polynomial regression\n",
    "\n",
    "#Polynomial Regression is a type of regression in machine learning \n",
    "# that models the relationship between variables as a polynomial\n",
    "\n",
    "#Y=b0+b1X+b2X2+...\n",
    "\n",
    "#It captures non-linear patterns by extending linear regression to fit curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24 How does polynomial regression differ from linear regression-\n",
    "#Linear Regression fits a straight line to the data, assuming a linear relationship between variables.\n",
    "\n",
    "#Polynomial Regression fits a curved line by adding polynomial terms (to capture non-linear patterns.)\n",
    "\n",
    "#Polynomial regression models more complex relationships that linear regression cannot handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#25-When is polynomial regression used?\n",
    "\n",
    "#Polynomial Regression is used when:\n",
    "\n",
    "#The relationship between the independent and dependent variables is non-linear.\n",
    "\n",
    "#A straight line (linear regression) cannot fit the data accurately.\n",
    "\n",
    "#It’s needed to model curved trends, like growth rates or cyclical patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#26-What is the general equation for polynomial regression\n",
    "#Y=b0+b1X+b2X2+..........+bnXn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#27-Can polynomial regression be applied to multiple variables\n",
    "\n",
    "#Yes, polynomial regression can be applied to multiple variables by including polynomial terms for each variable \n",
    "\n",
    "# This is called multivariate polynomial regression and models complex interactions between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#28-What are the limitations of polynomial regression\n",
    "\n",
    "#The limitations of polynomial regression are:\n",
    "\n",
    "#Overfitting: High-degree polynomials can fit the noise in the data, reducing generalization.\n",
    "\n",
    "#Complexity: Adding higher-degree terms makes the model harder to interpret.\n",
    "\n",
    "#Extrapolation issues: Predictions outside the data range can be highly inaccurate.\n",
    "\n",
    "#Computational cost: More terms increase computation, especially with many variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#29-What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
    "#To evaluate model fit when selecting the degree of a polynomial, you can use:\n",
    "\n",
    "#Cross-Validation: Split the data and test the model on unseen data to avoid overfitting.\n",
    "\n",
    "#Residual Plots: Check if residuals are randomly distributed (no patterns).\n",
    "\n",
    "#R² and Adjusted R²: Assess how well the model explains the variance without overfitting.\n",
    "\n",
    "#Mean Squared Error (MSE): Lower MSE indicates better fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30- Why is visualization important in polynomial regression\n",
    "#Visualization is important in polynomial regression because it helps understand the model's fit to the data \n",
    "\n",
    "# and ensures it captures trends accurately. \n",
    "\n",
    "# It also allows you to detect issues like overfitting or poor extrapolation visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#31- How is polynomial regression implemented in Python\n",
    "\n",
    "#Generate Polynomial Features: Use PolynomialFeatures from sklearn.preprocessing to create higher-degree terms \n",
    "\n",
    "# Fit a Model: Use LinearRegression from sklearn.linear_model to fit the transformed data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

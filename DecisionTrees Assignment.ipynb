{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-What is a Decision Tree, and how does it work?\n",
    "\n",
    "#A Decision Tree is a flowchart-like structure used for decision-making and predictions. \n",
    "\n",
    "# It splits data into branches based on conditions, leading to different outcomes.\n",
    "\n",
    "#  It works by recursively dividing data based on the feature that provides the best separation until reaching a decision (leaf node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2- What are impurity measures in Decision Trees?\n",
    "\n",
    "#Impurity measures in Decision Trees determine how mixed the data is at a node. \n",
    "\n",
    "# Common ones include Gini Impurity (measures misclassification) and Entropy (measures randomness).\n",
    "\n",
    "#  Lower impurity means purer splits, helping the tree make better decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-What is the mathematical formula for Gini Impurity?\n",
    "\n",
    "#The Gini Impurity formula is:\n",
    "\n",
    "#   1-summation pi^2\n",
    "\n",
    "#where is the proportion of class i in the node. A lower Gini value means a purer split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4- What is the mathematical formula for Entropy\n",
    "\n",
    "#The Entropy formula is:\n",
    "\n",
    "   #   H=- summation pilog pi base2\n",
    "#is the proportion of class i in the node. Lower entropy means a purer split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5- What is Information Gain, and how is it used in Decision Trees\n",
    "\n",
    "#Information Gain (IG) measures how much uncertainty (entropy)\n",
    "\n",
    "#  reduced after splitting a node in a Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6-What is the difference between Gini Impurity and Entropy\n",
    "\n",
    "#Gini Impurity measures the chance of incorrect classification, while Entropy measures the randomness in data.\n",
    "\n",
    "#  Gini is faster to compute, while Entropy is more sensitive to changes.\n",
    "\n",
    "#  Both help in finding the best splits in a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7-What is the mathematical explanation behind Decision Trees\n",
    "\n",
    "#Decision Trees split data by selecting the feature that maximizes Information Gain (IG) or minimizes Gini Impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-* What is Pre-Pruning in Decision Trees\n",
    "\n",
    "#Pre-Pruning stops a Decision Tree from growing too deep by setting limits like maximum depth or minimum samples per split. \n",
    "\n",
    "# This prevents overfitting by stopping splits that add little value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9-What is Post-Pruning in Decision Trees\t\n",
    "\n",
    "#Post-Pruning trims a fully grown Decision Tree by removing less important branches after training.\n",
    "\n",
    "#  This helps reduce overfitting and improves the modelâ€™s ability to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-* What is the difference between Pre-Pruning and Post-Pruning\n",
    "\n",
    "#Pre-Pruning stops tree growth early using conditions like max depth, while Post-Pruning trims an already grown tree by removing weak branches.\n",
    " \n",
    "#  Pre-pruning prevents overfitting from the start, while post-pruning fixes overfitting after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11-#111-What is a Decision Tree Regressor\n",
    "\n",
    "#A Decision Tree Regressor predicts continuous values instead of classes. \n",
    "\n",
    "# It splits data based on features to minimize errors (e.g., Mean Squared Error) and \n",
    "\n",
    "# makes predictions using the average value of data points in each leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12-What are the advantages and disadvantages of Decision Trees\n",
    "\n",
    "#Advantages: Easy to understand, handles both numerical & categorical data, and requires little data preprocessing.\n",
    "\n",
    "#Disadvantages: Prone to overfitting, sensitive to small changes in data, and can create biased trees if data is imbalanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13- How does a Decision Tree handle missing values\n",
    "\n",
    "#A Decision Tree handles missing values by either ignoring them during splitting,\n",
    "\n",
    "#  imputing them with mean/median/mode, or using surrogate splits (finding the next best feature to split on).\n",
    "\n",
    "#  This helps maintain accuracy without dropping data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14- How does a Decision Tree handle categorical features\n",
    "\n",
    "#A Decision Tree handles categorical features by splitting based on unique category values using techniques like one-hot encoding or label encoding.\n",
    " \n",
    "# It selects the best split using Gini Impurity or Entropy to improve predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15-* What are some real-world applications of Decision Trees\n",
    "\n",
    "#Decision Trees are used in credit risk assessment (approving loans), \n",
    "\n",
    "# medical diagnosis (predicting diseases), fraud detection (spotting suspicious transactions), \n",
    "\n",
    "# and customer segmentation (targeted marketing). They help make clear, rule-based decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#PRACTICAL\n",
    "#16-Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
    "\n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "# Load Iris dataset  \n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "# Split data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  \n",
    "\n",
    "# Train Decision Tree Classifier  \n",
    "clf = DecisionTreeClassifier()  \n",
    "clf.fit(X_train, y_train)  \n",
    "\n",
    "# Make predictions  \n",
    "y_pred = clf.predict(X_test)  \n",
    "\n",
    "# Print accuracy  \n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: [0.00752037 0.01880092 0.58454592 0.38913279]\n"
     ]
    }
   ],
   "source": [
    "#17-Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
    "#feature importances\n",
    "\n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "\n",
    "# Load Iris dataset  \n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "# Split data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  \n",
    "\n",
    "# Train Decision Tree Classifier with Gini Impurity  \n",
    "clf = DecisionTreeClassifier(criterion=\"gini\")  \n",
    "clf.fit(X_train, y_train)  \n",
    "\n",
    "# Print feature importances  \n",
    "print(\"Feature Importances:\", clf.feature_importances_)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#18- Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
    "#model accuracy  \n",
    "\n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "# Load Iris dataset  \n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "# Split data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  \n",
    "\n",
    "# Train Decision Tree Classifier with Entropy  \n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\")  \n",
    "clf.fit(X_train, y_train)  \n",
    "\n",
    "# Make predictions  \n",
    "y_pred = clf.predict(X_test)  \n",
    "\n",
    "# Print model accuracy  \n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5008313149894864\n"
     ]
    }
   ],
   "source": [
    "#19-Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
    "#Squared Error (MSE)\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.metrics import mean_squared_error  \n",
    "\n",
    "# Load California housing dataset  \n",
    "housing = fetch_california_housing()  \n",
    "X, y = housing.data, housing.target  \n",
    "\n",
    "# Split data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "\n",
    "# Train Decision Tree Regressor  \n",
    "regressor = DecisionTreeRegressor()  \n",
    "regressor.fit(X_train, y_train)  \n",
    "\n",
    "# Make predictions  \n",
    "y_pred = regressor.predict(X_test)  \n",
    "\n",
    "# Compute and print Mean Squared Error (MSE)  \n",
    "mse = mean_squared_error(y_test, y_pred)  \n",
    "print(\"Mean Squared Error:\", mse)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20-Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz*\n",
    "\n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz  \n",
    "import graphviz  \n",
    "\n",
    "# Load Iris dataset  \n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "# Split data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "\n",
    "# Train Decision Tree Classifier  \n",
    "clf = DecisionTreeClassifier()  \n",
    "clf.fit(X_train, y_train)  \n",
    "\n",
    "# Visualize the Decision Tree using Graphviz  \n",
    "dot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names,  \n",
    " class_names=iris.target_names, filled=True, rounded=True, special_characters=True)  \n",
    "\n",
    "# Render the tree  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.view()  # Opens the tree visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with max depth 3: 0.9666666666666667\n",
      "Accuracy with fully grown tree: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#21-* Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
    "#accuracy with a fully grown tree\n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "# Load Iris dataset  \n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "# Split data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  \n",
    "\n",
    "# Train Decision Tree with max depth of 3  \n",
    "clf_limited = DecisionTreeClassifier(max_depth=3)  \n",
    "clf_limited.fit(X_train, y_train)  \n",
    "y_pred_limited = clf_limited.predict(X_test)  \n",
    "accuracy_limited = accuracy_score(y_test, y_pred_limited)  \n",
    "\n",
    "# Train a fully grown Decision Tree  \n",
    "clf_full = DecisionTreeClassifier()  \n",
    "clf_full.fit(X_train, y_train)  \n",
    "y_pred_full = clf_full.predict(X_test)  \n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)  \n",
    "\n",
    "# Print accuracy comparison  \n",
    "print(\"Accuracy with max depth 3:\", accuracy_limited)  \n",
    "print(\"Accuracy with fully grown tree:\", accuracy_full)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with min_samples_split=5: 0.9666666666666667\n",
      "Accuracy with default tree: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#22-Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
    "#accuracy with a default tree*\n",
    "\n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "# Load Iris dataset  \n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "# Split data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  \n",
    "\n",
    "# Train Decision Tree with min_samples_split=5  \n",
    "clf_limited = DecisionTreeClassifier(min_samples_split=5)  \n",
    "clf_limited.fit(X_train, y_train)  \n",
    "y_pred_limited = clf_limited.predict(X_test)  \n",
    "accuracy_limited = accuracy_score(y_test, y_pred_limited)  \n",
    "\n",
    "# Train a default Decision Tree  \n",
    "clf_default = DecisionTreeClassifier()  \n",
    "clf_default.fit(X_train, y_train)  \n",
    "y_pred_default = clf_default.predict(X_test)  \n",
    "accuracy_default = accuracy_score(y_test, y_pred_default)  \n",
    "\n",
    "# Print accuracy comparison  \n",
    "print(\"Accuracy with min_samples_split=5:\", accuracy_limited)  \n",
    "print(\"Accuracy with default tree:\", accuracy_default)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with unscaled data: 0.9666666666666667\n",
      "Accuracy with scaled data: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#23-Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
    "#accuracy with unscaled data\n",
    "\n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "# Load Iris dataset  \n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "# Split data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  \n",
    "\n",
    "# Train Decision Tree on unscaled data  \n",
    "clf_unscaled = DecisionTreeClassifier()  \n",
    "clf_unscaled.fit(X_train, y_train)  \n",
    "y_pred_unscaled = clf_unscaled.predict(X_test)  \n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)  \n",
    "\n",
    "# Apply feature scaling (Standardization)  \n",
    "scaler = StandardScaler()  \n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)  \n",
    "\n",
    "# Train Decision Tree on scaled data  \n",
    "clf_scaled = DecisionTreeClassifier()  \n",
    "clf_scaled.fit(X_train_scaled, y_train)  \n",
    "y_pred_scaled = clf_scaled.predict(X_test_scaled)  \n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)  \n",
    "\n",
    "# Print accuracy comparison  \n",
    "print(\"Accuracy with unscaled data:\", accuracy_unscaled)  \n",
    "print(\"Accuracy with scaled data:\", accuracy_scaled)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy using One-vs-Rest: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#24-Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
    "#classification*\n",
    "\n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.multiclass import OneVsRestClassifier  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "# Load Iris dataset  \n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "# Split data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  \n",
    "\n",
    "# Train Decision Tree Classifier using One-vs-Rest (OvR) strategy  \n",
    "clf_ovr = OneVsRestClassifier(DecisionTreeClassifier())  \n",
    "clf_ovr.fit(X_train, y_train)  \n",
    "\n",
    "# Make predictions  \n",
    "y_pred = clf_ovr.predict(X_test)  \n",
    "\n",
    "# Print model accuracy  \n",
    "print(\"Model Accuracy using One-vs-Rest:\", accuracy_score(y_test, y_pred))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm): 0.0075\n",
      "sepal width (cm): 0.0313\n",
      "petal length (cm): 0.5720\n",
      "petal width (cm): 0.3891\n"
     ]
    }
   ],
   "source": [
    "#25-* Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
    "\n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "\n",
    "# Load Iris dataset  \n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "# Split data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  \n",
    "\n",
    "# Train Decision Tree Classifier  \n",
    "clf = DecisionTreeClassifier()  \n",
    "clf.fit(X_train, y_train)  \n",
    "\n",
    "# Display feature importance scores  \n",
    "for feature, importance in zip(iris.feature_names, clf.feature_importances_):  \n",
    "     print(f\"{feature}: {importance:.4f}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

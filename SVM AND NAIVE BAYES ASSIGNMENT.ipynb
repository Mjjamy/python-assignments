{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-What is a Support Vector Machine (SVM)?\n",
    "\n",
    "#A Support Vector Machine (SVM) is a machine learning algorithm used for classification and regression.\n",
    "# It finds the best boundary (hyperplane) that separates data into different classes. SVM works well for complex datasets, \n",
    "# even when data isn’t perfectly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-What is the difference between Hard Margin and Soft Margin SVM?\n",
    "\n",
    "#Hard Margin SVM strictly separates classes with a clear boundary and works only when data is perfectly separable. \n",
    "\n",
    "# Soft Margin SVM allows some misclassification to handle overlapping data, making it more flexible for real-world problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-What is the mathematical intuition behind SVM\n",
    "\n",
    "#SVM finds the best boundary (hyperplane) that maximizes the margin between different classes.\n",
    " \n",
    "# It uses mathematical optimization to find this hyperplane by minimizing errors and maximizing the distance from the closest data points (support vectors).\n",
    "\n",
    "#  For non-linear data, it transforms the data using a kernel trick to make it separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4-What is the role of Lagrange Multipliers in SVM\n",
    "\n",
    "#Lagrange Multipliers help SVM find the best hyperplane by converting the constrained optimization problem into an easier one to solve.\n",
    " \n",
    "# They ensure that only the most important data points (support vectors) influence the decision boundary while keeping computations efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-What are Support Vectors in SVM\n",
    "#Support Vectors are the key data points in SVM that are closest to the decision boundary.\n",
    "#  They define the optimal hyperplane and help maximize the margin between classes. Removing them would change the boundary,\n",
    "#  making them crucial for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6-What is a Support Vector Classifier (SVC)\n",
    "\n",
    "# A Support Vector Classifier (SVC) is an SVM used for classification tasks. \n",
    "# It finds the best boundary (hyperplane) that separates different classes while allowing some misclassification (Soft Margin) for better flexibility.\n",
    "#  SVC works well with both linear and non-linear data using kernel tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7-What is a Support Vector Regressor (SVR)\n",
    "\n",
    "#A Support Vector Regressor (SVR) is an SVM used for regression tasks.\n",
    "#  Instead of finding a boundary, it finds a line (or curve) that fits the data while keeping most points within a margin.\n",
    "#  It’s useful for handling outliers and capturing complex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-What is the Kernel Trick in SVM\n",
    "\n",
    "#The Kernel Trick allows SVM to handle complex, non-linear data by transforming it into a higher-dimensional space where it becomes easier to separate.\n",
    "#  Instead of manually computing this transformation, the kernel function does it efficiently, making SVM more powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9-Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
    "\n",
    "#Linear Kernel: Used when data is linearly separable; fast and simple.\n",
    "#Polynomial Kernel: Captures more complex patterns by mapping data into a higher-degree polynomial space.\n",
    "#RBF (Radial Basis Function) Kernel: Best for highly non-linear data; it maps points into infinite-dimensional space for better separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-What is the effect of the C parameter in SVM\n",
    "\n",
    "#The C parameter in SVM controls the trade-off between a smooth decision boundary and correctly classifying training points. \n",
    "\n",
    "# A high C tries to classify all points correctly (less margin, risk of overfitting), \n",
    "# while a low C allows some misclassification for a more general boundary (better generalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11-What is the role of the Gamma parameter in RBF Kernel SVM\n",
    "\n",
    "#The Gamma parameter in RBF Kernel SVM controls how far a single data point's influence reaches.\n",
    "#  A high gamma makes the model focus on nearby points (risk of overfitting),\n",
    "#  while a low gamma considers distant points, creating a smoother decision boundary (better generalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12-What is the Naïve Bayes classifier, and why is it called \"Naïve\"\n",
    "\n",
    "#Naïve Bayes is a simple and fast classification algorithm based on Bayes' theorem.\n",
    "#  It assumes that all features are independent, which is rarely true in real life—this unrealistic assumption makes it \"naïve.\"\n",
    "#  Despite this, it works well for text classification and spam filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13-What is Bayes’ Theorem\n",
    "\n",
    "#Bayes' Theorem calculates the probability of an event based on prior knowledge of related events. It updates beliefs when new evidence appears and is widely used in machine learning and statistics. Formula:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14-Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
    "\n",
    "#Gaussian Naïve Bayes: Used for continuous data, assumes features follow a normal (Gaussian) distribution.\n",
    "\n",
    "#Multinomial Naïve Bayes: Best for text data (e.g., word counts), works well for classification tasks like spam detection.\n",
    "\n",
    "#Bernoulli Naïve Bayes: Used for binary data (0s and 1s), suitable for tasks like document classification based on word presence/absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15- When should you use Gaussian Naïve Bayes over other variants\n",
    "\n",
    "#Use Gaussian Naïve Bayes when your features are continuous and follow a normal (Gaussian) distribution. \n",
    "# It works well for datasets with numerical values like age, height, or test scores. \n",
    "# If your data is categorical or text-based, other variants like Multinomial or Bernoulli Naïve Bayes are better choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16-What are the key assumptions made by Naïve Bayes\n",
    "\n",
    "#Naïve Bayes makes two key assumptions:\n",
    "\n",
    "#Feature Independence – It assumes that all features are independent and do not affect each other, which is rarely true but simplifies calculations.\n",
    "\n",
    "#Class Conditional Independence – Given the class label, the features are assumed to be independent of each other.\n",
    "\n",
    "#Despite these assumptions, Naïve Bayes works well in many real-world applications like spam filtering and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17-What are the advantages and disadvantages of Naïve Bayes\n",
    "\n",
    "#dvantages:\n",
    "#Fast, simple, and works well with small or large data.\n",
    "#Performs great on text classification (e.g., spam filtering).\n",
    "#Can handle missing data and requires less training time.\n",
    "\n",
    "#Disadvantages:\n",
    "#Assumes features are independent, which is often unrealistic.\n",
    "#Struggles with highly correlated features.\n",
    "#Zero probability issue if a category is missing (fixed by smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18- Why is Naïve Bayes a good choice for text classification\n",
    "\n",
    "#Naïve Bayes is great for text classification because:\n",
    "\n",
    "#Handles High-Dimensional Data – Works well with large vocabularies in text data.\n",
    "\n",
    "#Fast and Efficient – Requires less training time, even on big datasets.\n",
    "\n",
    "#Performs Well with Sparse Data – Effective for documents with many zero values (e.g., word counts).\n",
    "\n",
    "#Works Well with Probabilities – Assigns likelihood scores to different categories, improving classification.\n",
    "\n",
    "#It's widely used for spam detection, sentiment analysis, and topic categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19-Compare SVM and Naïve Bayes for classification tasks\n",
    "\n",
    "#SVM: Best for complex, high-dimensional data, but slower.\n",
    "\n",
    "#Naïve Bayes: Fast and great for text classification.\n",
    "\n",
    "#SVM: Works well with non-linear patterns using kernel tricks.\n",
    "\n",
    "#Naïve Bayes: Assumes feature independence, which is often unrealistic.\n",
    "\n",
    "#SVM: More robust to noise, but computationally expensive.\n",
    "\n",
    "#Naïve Bayes: Handles sparse data well, like word counts in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20-How does Laplace Smoothing help in Naïve Bayes\n",
    "\n",
    "#Laplace Smoothing prevents zero probability issues in Naïve Bayes when a category is missing in the training data.\n",
    "#  It adds a small value (usually 1) to all counts,\n",
    "#  ensuring every word or feature has a nonzero probability. This helps improve model accuracy, especially for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21-Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Create and train the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')  # You can change kernel to 'rbf', 'poly', etc.\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22-Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
    "#compare their accuracies\n",
    "\n",
    "#from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel Accuracy: 0.94\n",
      "RBF Kernel Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Train SVM classifier with Linear kernel\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_linear.fit(X_train, y_train)\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "\n",
    "# Train SVM classifier with RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# Compare accuracies\n",
    "print(f'Linear Kernel Accuracy: {accuracy_linear:.2f}')\n",
    "print(f'RBF Kernel Accuracy: {accuracy_rbf:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#23-Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
    "#Squared Error (MSE)\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = datasets.fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.33\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train SVM Regressor (SVR)\n",
    "svr_model = SVR(kernel='rbf')  # You can change the kernel to 'linear', 'poly', etc.\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svr_model.predict(X_test)\n",
    "\n",
    "# Evaluate using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24-%: Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
    "#boundary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Load a toy dataset (for easy visualization)\n",
    "X, y = datasets.make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM classifier with Polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_regions(X, y, clf=svm_poly, legend=2)\n",
    "plt.title(\"SVM with Polynomial Kernel\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#25-Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
    "#evaluate accuracy:\n",
    "\n",
    "#from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Train Gaussian Naïve Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#26-Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
    "#Newsgroups dataset.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=['rec.sport.baseball', 'sci.space', 'comp.graphics'], remove=('headers', 'footers', 'quotes'))\n",
    "X, y = newsgroups.data, newsgroups.target\n",
    "\n",
    "# Convert text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "X_counts = vectorizer.fit_transform(X)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX_tfidf\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train Multinomial Naïve Bayes classifier\u001b[39;00m\n\u001b[0;32m      5\u001b[0m nb_classifier \u001b[38;5;241m=\u001b[39m MultinomialNB()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Multinomial Naïve Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_decision_regions\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load a toy dataset\u001b[39;00m\n\u001b[0;32m     11\u001b[0m X, y \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mmake_moons(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "#27- Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
    "#boundaries visually\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Load a toy dataset\n",
    "X, y = datasets.make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different C values to test\n",
    "C_values = [0.1, 1, 10]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, C in enumerate(C_values, 1):\n",
    "    # Train SVM classifier with different C values\n",
    "    svm_model = SVC(kernel='linear', C=C)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.subplot(1, 3, i)\n",
    "    plot_decision_regions(X, y, clf=svm_model, legend=2)\n",
    "    plt.title(f'SVM with C={C}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "#28-= Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
    "#binary features\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Create a synthetic dataset with binary features\n",
    "X, y = make_classification(n_samples=500, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
    "X = np.where(X > 0, 1, 0)  # Convert features to binary values (0 or 1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Bernoulli Naïve Bayes classifier\n",
    "bnb_classifier = BernoulliNB()\n",
    "bnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bnb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#29-Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
    "#unscaled data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without scaling: 1.00\n",
      "Accuracy with scaling: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM without feature scaling\n",
    "svm_unscaled = SVC(kernel='linear')\n",
    "svm_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
    "unscaled_accuracy = accuracy_score(y_test, y_pred_unscaled)\n",
    "\n",
    "# Apply feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM with feature scaling\n",
    "svm_scaled = SVC(kernel='linear')\n",
    "svm_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
    "scaled_accuracy = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# Compare results\n",
    "print(f'Accuracy without scaling: {unscaled_accuracy:.2f}')\n",
    "print(f'Accuracy with scaling: {scaled_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without Laplace Smoothing: 1.00\n",
      "Accuracy with Laplace Smoothing: 1.00\n"
     ]
    }
   ],
   "source": [
    "#30-= Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and\n",
    "#after Laplace Smoothing\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Gaussian Naïve Bayes without Laplace Smoothing\n",
    "gnb_no_smoothing = GaussianNB(var_smoothing=0)  # Essentially no smoothing\n",
    "gnb_no_smoothing.fit(X_train, y_train)\n",
    "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
    "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
    "\n",
    "# Train Gaussian Naïve Bayes with default Laplace Smoothing\n",
    "gnb_smoothing = GaussianNB()  # Default var_smoothing\n",
    "gnb_smoothing.fit(X_train, y_train)\n",
    "y_pred_smoothing = gnb_smoothing.predict(X_test)\n",
    "accuracy_smoothing = accuracy_score(y_test, y_pred_smoothing)\n",
    "\n",
    "# Compare results\n",
    "print(f'Accuracy without Laplace Smoothing: {accuracy_no_smoothing:.2f}')\n",
    "print(f'Accuracy with Laplace Smoothing: {accuracy_smoothing:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
